{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from numpy import array, linspace\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from matplotlib.pyplot import plot\n",
    "from scipy.signal import argrelextrema\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from handlers.DataHandler import DataHandler\n",
    "# from handlers.AgglomerativeHandler import AgglomerativeHandler as ch\n",
    "from handlers.OPTICSHandler import OPTICSHandler as ch\n",
    "from handlers import GAHandler\n",
    "from handlers import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL VARIABLES ###\n",
    "y_horizon = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'META', 'GOOGL', 'GOOG',\n",
    "       'LLY', 'JPM', 'AVGO', 'TSLA', 'UNH', 'XOM', 'V', 'PG', 'JNJ', 'MA',\n",
    "       'COST', 'HD', 'ABBV', 'WMT', 'MRK', 'NFLX', 'KO', 'BAC', 'ADBE',\n",
    "       'PEP', 'CVX', 'CRM', 'TMO', 'ORCL', 'LIN', 'AMD', 'ACN', 'MCD',\n",
    "       'ABT', 'CSCO', 'PM', 'WFC', 'IBM', 'TXN', 'QCOM', 'GE', 'DHR',\n",
    "       'VZ', 'INTU', 'AMGN', 'NOW', 'ISRG', 'NEE', 'SPGI', 'PFE', 'CAT',\n",
    "       'DIS', 'RTX', 'CMCSA', 'GS', 'UNP', 'T', 'AMAT', 'PGR',\n",
    "       'LOW', 'AXP', 'TJX', 'HON', 'BKNG', 'ELV', 'COP', 'SYK', 'MS',\n",
    "       'LMT', 'VRTX', 'BLK', 'REGN', 'MDT', 'BSX', 'PLD', 'CB', 'ETN',\n",
    "       'C', 'MMC', 'ADP', 'AMT', 'PANW', 'ADI', 'SBUX', 'MDLZ', 'CI',\n",
    "       'TMUS', 'FI', 'BMY', 'DE', 'GILD', 'BX', 'NKE', 'SO', 'LRCX', 'MU', 'KLAC', 'SCHW']\n",
    "\n",
    "top10 = top100[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-22 16:29:13,241 - handlers.DataHandler - INFO - Loading data for AAPL from cache\n",
      "2024-09-22 16:29:13,383 - handlers.DataHandler - INFO - Loading data for MSFT from cache\n",
      "2024-09-22 16:29:13,508 - handlers.DataHandler - INFO - Loading data for NVDA from cache\n",
      "2024-09-22 16:29:13,633 - handlers.DataHandler - INFO - Loading data for AMZN from cache\n",
      "2024-09-22 16:29:13,758 - handlers.DataHandler - INFO - Loading data for META from cache\n",
      "2024-09-22 16:29:13,873 - handlers.DataHandler - INFO - Loading data for GOOGL from cache\n",
      "2024-09-22 16:29:13,994 - handlers.DataHandler - INFO - Loading data for GOOG from cache\n",
      "2024-09-22 16:29:14,115 - handlers.DataHandler - INFO - Loading data for LLY from cache\n",
      "2024-09-22 16:29:14,241 - handlers.DataHandler - INFO - Loading data for JPM from cache\n",
      "2024-09-22 16:29:14,369 - handlers.DataHandler - INFO - Loading data for AVGO from cache\n",
      "2024-09-22 16:29:50,962 - handlers.OPTICSHandler - INFO - daily_log_return_0 had 2 clusters\n",
      "2024-09-22 16:30:27,490 - handlers.OPTICSHandler - INFO - daily_log_return_1 had 2 clusters\n",
      "2024-09-22 16:31:03,853 - handlers.OPTICSHandler - INFO - daily_log_return_2 had 2 clusters\n",
      "2024-09-22 16:31:39,957 - handlers.OPTICSHandler - INFO - daily_log_return_3 had 2 clusters\n",
      "2024-09-22 16:32:15,849 - handlers.OPTICSHandler - INFO - daily_log_return_4 had 2 clusters\n",
      "2024-09-22 16:32:51,718 - handlers.OPTICSHandler - INFO - daily_log_return_5 had 2 clusters\n",
      "2024-09-22 16:33:27,615 - handlers.OPTICSHandler - INFO - daily_log_return_6 had 2 clusters\n",
      "2024-09-22 16:34:03,726 - handlers.OPTICSHandler - INFO - daily_log_return_7 had 2 clusters\n",
      "2024-09-22 16:34:39,563 - handlers.OPTICSHandler - INFO - daily_log_return_8 had 2 clusters\n",
      "2024-09-22 16:35:15,180 - handlers.OPTICSHandler - INFO - daily_log_return_9 had 2 clusters\n",
      "2024-09-22 16:35:50,889 - handlers.OPTICSHandler - INFO - daily_log_return_10 had 2 clusters\n",
      "2024-09-22 16:36:26,667 - handlers.OPTICSHandler - INFO - daily_log_return_11 had 2 clusters\n",
      "2024-09-22 16:37:02,323 - handlers.OPTICSHandler - INFO - daily_log_return_12 had 2 clusters\n",
      "2024-09-22 16:37:38,094 - handlers.OPTICSHandler - INFO - intraday_log_return_0 had 1 clusters\n",
      "2024-09-22 16:38:13,689 - handlers.OPTICSHandler - INFO - intraday_log_return_1 had 1 clusters\n",
      "2024-09-22 16:38:49,150 - handlers.OPTICSHandler - INFO - intraday_log_return_2 had 1 clusters\n",
      "2024-09-22 16:39:24,966 - handlers.OPTICSHandler - INFO - intraday_log_return_3 had 1 clusters\n",
      "2024-09-22 16:40:00,460 - handlers.OPTICSHandler - INFO - intraday_log_return_4 had 1 clusters\n",
      "2024-09-22 16:40:35,983 - handlers.OPTICSHandler - INFO - intraday_log_return_5 had 1 clusters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create and use the Cluster object\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m ch(train_X\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mstationary_cols \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]), train_y\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[1;32m     17\u001b[0m                      test_X\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mstationary_cols \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]), test_y\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[1;32m     18\u001b[0m                      max_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m clustered_train_X, clustered_train_y, clustered_test_X, clustered_test_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Access the clustering results\u001b[39;00m\n\u001b[1;32m     23\u001b[0m feature_cluster_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_cluster_stats\n",
      "File \u001b[0;32m~/fyp/handlers/OPTICSHandler.py:50\u001b[0m, in \u001b[0;36mOPTICSHandler.cluster\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Cluster each feature column\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_X\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 50\u001b[0m     temp, test_temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     train_X_res\u001b[38;5;241m.\u001b[39mappend(temp)\n\u001b[1;32m     52\u001b[0m     test_X_res\u001b[38;5;241m.\u001b[39mappend(test_temp)\n",
      "File \u001b[0;32m~/fyp/handlers/OPTICSHandler.py:72\u001b[0m, in \u001b[0;36mOPTICSHandler._process_column\u001b[0;34m(self, column_name, is_target)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column_name, is_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Process a single column for clustering.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     temp, testTemp, clsDic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cluster_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Remove temporary columns\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     temp\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster_Mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster_Std\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/fyp/handlers/OPTICSHandler.py:100\u001b[0m, in \u001b[0;36mOPTICSHandler._cluster_column\u001b[0;34m(self, column_name, is_target)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Perform OPTICS clustering\u001b[39;00m\n\u001b[1;32m     99\u001b[0m clusterer \u001b[38;5;241m=\u001b[39m OPTICS(min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples, max_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_eps)\n\u001b[0;32m--> 100\u001b[0m cls1 \u001b[38;5;241m=\u001b[39m \u001b[43mclusterer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Handle noise points (labeled as -1 by OPTICS)\u001b[39;00m\n\u001b[1;32m    103\u001b[0m cls1[cls1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cls1) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/fyp1/lib/python3.12/site-packages/sklearn/base.py:900\u001b[0m, in \u001b[0;36mClusterMixin.fit_predict\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03mPerform clustering on `X` and returns cluster labels.\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;124;03m    Cluster labels.\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/.conda/envs/fyp1/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fyp1/lib/python3.12/site-packages/sklearn/cluster/_optics.py:350\u001b[0m, in \u001b[0;36mOPTICS.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    342\u001b[0m         X\u001b[38;5;241m.\u001b[39msetdiag(X\u001b[38;5;241m.\u001b[39mdiagonal())\n\u001b[1;32m    343\u001b[0m memory \u001b[38;5;241m=\u001b[39m check_memory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory)\n\u001b[1;32m    345\u001b[0m (\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordering_,\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_distances_,\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreachability_,\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredecessor_,\n\u001b[0;32m--> 350\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_optics_graph\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleaf_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaf_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Extract clusters from the calculated orders and reachability\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/fyp1/lib/python3.12/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fyp1/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/fyp1/lib/python3.12/site-packages/sklearn/cluster/_optics.py:647\u001b[0m, in \u001b[0;36mcompute_optics_graph\u001b[0;34m(X, min_samples, max_eps, metric, p, metric_params, algorithm, leaf_size, n_jobs)\u001b[0m\n\u001b[1;32m    645\u001b[0m     ordering[ordering_idx] \u001b[38;5;241m=\u001b[39m point\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m core_distances_[point] \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39minf:\n\u001b[0;32m--> 647\u001b[0m         \u001b[43m_set_reach_dist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcore_distances_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_distances_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreachability_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreachability_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredecessor_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredecessor_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpoint_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprocessed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnbrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misinf(reachability_)):\n\u001b[1;32m    661\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    662\u001b[0m         (\n\u001b[1;32m    663\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll reachability values are inf. Set a larger\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    667\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/fyp1/lib/python3.12/site-packages/sklearn/cluster/_optics.py:712\u001b[0m, in \u001b[0;36m_set_reach_dist\u001b[0;34m(core_distances_, reachability_, predecessor_, point_index, processed, X, nbrs, metric, metric_params, p, max_eps)\u001b[0m\n\u001b[1;32m    710\u001b[0m rdists \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(dists, core_distances_[point_index])\n\u001b[1;32m    711\u001b[0m np\u001b[38;5;241m.\u001b[39maround(rdists, decimals\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfinfo(rdists\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mprecision, out\u001b[38;5;241m=\u001b[39mrdists)\n\u001b[0;32m--> 712\u001b[0m improved \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(rdists \u001b[38;5;241m<\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreachability_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munproc\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    713\u001b[0m reachability_[unproc[improved]] \u001b[38;5;241m=\u001b[39m rdists[improved]\n\u001b[1;32m    714\u001b[0m predecessor_[unproc[improved]] \u001b[38;5;241m=\u001b[39m point_index\n",
      "File \u001b[0;32m~/.conda/envs/fyp1/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:207\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(a, indices, axis, out, mode)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_take_dispatcher)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(a, indices, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    Take elements from an array along an axis.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m           [5, 7]])\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtake\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fyp1/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_X, train_y, test_X, test_y = DataHandler.getTickers(top100, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "train_X, train_y, test_X, test_y = DataHandler.getTickers(top10, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "# train_X, train_y, test_X, test_y = DataHandler.getData(\"MS\", \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2023-01-01\", y_horizon)\n",
    "\n",
    "# train_X.drop(columns=['symbol'], inplace=True)\n",
    "# train_y.drop(columns=['symbol'], inplace=True)\n",
    "# test_X.drop(columns=['symbol'], inplace=True)\n",
    "# test_y.drop(columns=['symbol'], inplace=True)\n",
    "\n",
    "# train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "# test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "\n",
    "stationary_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'notional_traded']\n",
    "\n",
    "# Create and use the Cluster object\n",
    "cls = ch(train_X.drop(columns=stationary_cols + ['symbol']), train_y.drop(columns=['symbol']), \n",
    "                     test_X.drop(columns=stationary_cols + ['symbol']), test_y.drop(columns=['symbol']), \n",
    "                     max_eps=0.1, min_samples=0.01)\n",
    "\n",
    "clustered_train_X, clustered_train_y, clustered_test_X, clustered_test_y = cls.cluster()\n",
    "\n",
    "# Access the clustering results\n",
    "feature_cluster_stats = cls.feature_cluster_stats\n",
    "y_cluster_stats = cls.y_cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_train_X.drop(columns=clustered_train_X.columns[clustered_train_X.isna().any(axis=0)], inplace=True)\n",
    "clustered_test_X.drop(columns=clustered_test_X.columns[clustered_test_X.isna().any(axis=0)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_train_X.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_test_X.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clustered_test_X.columns == clustered_train_X.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads=10, ff_dim=32, num_transformer_blocks=4, mlp_units=256, dropout=0.25, noHiddenLayers=1, sigmoidOrSoftmax=0):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # print(f\"Initializing TransformerModel with input_dim: {input_dim}, output_dim: {output_dim}\")\n",
    "\n",
    "        # # Ensure input_dim is divisible by num_heads\n",
    "        # if input_dim % num_heads != 0:\n",
    "        #     new_input_dim = (input_dim // num_heads + 1) * num_heads\n",
    "        #     print(f\"Adjusting input_dim from {input_dim} to {new_input_dim} to be divisible by num_heads ({num_heads})\")\n",
    "        #     input_dim = new_input_dim\n",
    "\n",
    "        # Encoder layer with ff_dim and dropout\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=num_transformer_blocks)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # MLP layers\n",
    "        fcList = [torch.nn.Linear(input_dim, mlp_units), torch.nn.ReLU(), self.dropout]\n",
    "        for i in range(noHiddenLayers):\n",
    "            fcList.extend([\n",
    "                torch.nn.Linear(mlp_units, mlp_units//2),\n",
    "                torch.nn.ReLU(),\n",
    "                self.dropout\n",
    "            ])\n",
    "            mlp_units = mlp_units//2\n",
    "        fcList.append(torch.nn.Linear(mlp_units, output_dim))\n",
    "        \n",
    "        # Output activation\n",
    "        if sigmoidOrSoftmax == 0:\n",
    "            fcList.append(torch.nn.Sigmoid())\n",
    "        else:\n",
    "            fcList.append(torch.nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(*fcList)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_length, input_dim)\n",
    "        \n",
    "        # Pass input through the transformer encoder\n",
    "        encoder_output = self.transformer_encoder(src)\n",
    "        \n",
    "        # Apply dropout after the transformer encoder\n",
    "        encoder_output = self.dropout(encoder_output)\n",
    "        \n",
    "        # Pass through the MLP layers\n",
    "        output = self.fc(encoder_output)\n",
    "        return output\n",
    "    \n",
    "def increaseInstancesExtreme(train, thresholdToIncrease=0.03):\n",
    "    extraData = train[(train[f\"Close_t+{yTarget}\"]>thresholdToIncrease) | (train[f\"Close_t+{yTarget}\"]<-thresholdToIncrease)]\n",
    "    return pd.concat([train, extraData] ,axis = 0)\n",
    "\n",
    "def train_model(X, Y, X_test, Y_test, # fuzzified inputs\n",
    "                   Y_test_raw, # crisp value\n",
    "                   num_heads, feed_forward_dim, num_transformer_blocks, mlp_units, dropout_rate, \n",
    "                   learning_rate, num_mlp_layers, num_epochs, activation_function, batch_size):\n",
    "\n",
    "    OUTPUT_FREQ = 50\n",
    "    # Detect GPU availability\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    # Load and pad data\n",
    "    x_padded, x_test_padded = Utils.padData(X, X_test, math.ceil(X.shape[1] / num_heads) * num_heads - X.shape[1])\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TransformerModel(\n",
    "        input_dim=x_padded.shape[1], \n",
    "        output_dim=Y.shape[1],\n",
    "        num_heads=num_heads, \n",
    "        ff_dim=feed_forward_dim, \n",
    "        num_transformer_blocks=num_transformer_blocks, \n",
    "        mlp_units=mlp_units, \n",
    "        dropout=dropout_rate, \n",
    "        sigmoidOrSoftmax=activation_function\n",
    "    ).double()\n",
    "\n",
    "    # Send model to the detected device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set loss function and optimizer\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader for training\n",
    "    assert x_padded.shape[0] == Y.shape[0]\n",
    "    train_dataset = TensorDataset(torch.from_numpy(x_padded), torch.from_numpy(Y))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            # Move data to the correct device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Collect predictions and targets for R² score computation\n",
    "            all_preds.append(outputs.detach().cpu().numpy())\n",
    "            all_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "        # Concatenate all batch predictions and targets\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "        if epoch % OUTPUT_FREQ == 0:\n",
    "            # Compute R² score\n",
    "            r2 = r2_score(all_targets, all_preds)\n",
    "            log_return_r2 = eval_model(model, x_test_padded, Y_test, Y_test_raw)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss / len(train_dataloader):.4f} | Train Cluster R² Score: {r2:.4f} | Test Log Return R² Score: {log_return_r2:.4f}')\n",
    "\n",
    "    # return all_preds, all_targets\n",
    "\n",
    "    # Evaluate on test data\n",
    "    test_dataset = TensorDataset(torch.from_numpy(x_test_padded), torch.from_numpy(Y_test))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=512*4, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_dataloader:\n",
    "            pred = model(inputs.to(device)).cpu()\n",
    "            pred_list.append(pred)\n",
    "\n",
    "    mse = Utils.testData(np.concatenate(pred_list), Y_test)\n",
    "    # print(f'Test Data MSE: {mse:.7f}')\n",
    "\n",
    "    # Calculate R2 score\n",
    "    pred = np.concatenate(pred_list)\n",
    "    pred[np.isnan(pred)] = 0\n",
    "\n",
    "    pred_log_returns = cls.deFuzzify(pred, pred_col)\n",
    "    pred_log_returns = np.nan_to_num(pred_log_returns, nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    # pred_closing = addResToClosing(cls, res)[:-yTarget]\n",
    "    # actual_closing = cls.test.Close[yTarget:].to_numpy()\n",
    "    # r2_score_value = r2_score(actual_closing, pred_closing)\n",
    "    \n",
    "    r2_score_value = r2_score(Y_test_raw, pred_log_returns)\n",
    "    print(\"Test R2 Score:\", r2_score_value)\n",
    "\n",
    "    return model, r2_score_value, pred\n",
    "\n",
    "def eval_model(model, X: np.array, Y: np.array, Y_crisp: np.array):\n",
    "    \"\"\"\n",
    "    X & Y are fuzzified inputs\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(f\"Using {device}\")    \n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X), torch.from_numpy(Y))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=512*4, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_dataloader:\n",
    "            pred = model(inputs.to(device)).cpu()\n",
    "            pred_list.append(pred)\n",
    "\n",
    "    # mse = Utils.testData(np.concatenate(pred_list), Y_test)\n",
    "    # print(f'Test Data MSE: {mse:.7f}')\n",
    "\n",
    "    # Calculate R2 score\n",
    "    pred = np.concatenate(pred_list)\n",
    "    pred[np.isnan(pred)] = 0\n",
    "\n",
    "    pred_log_returns = cls.deFuzzify(pred, pred_col)\n",
    "    pred_log_returns = np.nan_to_num(pred_log_returns, nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    r2_score_value = Utils.custom_r2_score(Y_crisp, pred_log_returns)\n",
    "    # print(\"Test R2 Score:\", r2_score_value)    \n",
    "    return r2_score_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_col = test_y.columns[0]\n",
    "\n",
    "train_val_split = 0.7\n",
    "unique_train_dates = clustered_train_X.index.unique()\n",
    "split_date = unique_train_dates[int(len(unique_train_dates) * train_val_split)]\n",
    "\n",
    "\n",
    "X = clustered_train_X.loc[clustered_train_X.index < split_date].to_numpy()\n",
    "val_X = clustered_train_X.loc[clustered_train_X.index >= split_date].to_numpy()\n",
    "\n",
    "y = clustered_train_y[[c for c in clustered_train_y.columns if pred_col in c]].loc[clustered_train_y.index < split_date].to_numpy()\n",
    "val_y = clustered_train_y[[c for c in clustered_train_y.columns if pred_col in c]].loc[clustered_train_y.index >= split_date].to_numpy()\n",
    "crisp_val_y = train_y[pred_col].loc[train_y.index >= split_date].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the genetic algorithm\n",
    "best_chromosome, best_fitness = GAHandler.genetic_algorithm(\n",
    "    train_model, X, y, val_X, val_y, crisp_val_y, \n",
    "    population_size=50, final_population_size=5, generations=10, elite_size=2\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the results\n",
    "print(\"Best hyperparameters:\", best_chromosome)\n",
    "print(\"Best R2 score:\", best_fitness)\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "print(f\"Execution time: {execution_time/60:.4f} minutes\")\n",
    "\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "# final_model, final_r2, final_pred = train_model(\n",
    "#     X, y, t_X, t_y, crisp_t_y,\n",
    "#     **best_chromosome\n",
    "# )\n",
    "# print(\"Final model R2 score:\", final_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", best_chromosome)\n",
    "print(\"Best R2 score:\", best_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_chromosome = {\n",
    "    'num_heads': 3, 'feed_forward_dim': 64, 'num_transformer_blocks': 1, \n",
    "    'mlp_units': 512, 'dropout_rate': 0.2, 'learning_rate': 5e-05, \n",
    "    'num_mlp_layers': 8, 'num_epochs': 500, 'activation_function': 1, 'batch_size': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_chromosome['batch_size'] = 256\n",
    "# best_chromosome['num_epochs'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_X, train_y, test_X, test_y = DataHandler.getTickers(top100, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "train_X, train_y, test_X, test_y = DataHandler.getTickers(top10, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "# train_X, train_y, test_X, test_y = DataHandler.getData(\"MS\", \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2023-01-01\", y_horizon)\n",
    "\n",
    "# train_X.drop(columns=['symbol'], inplace=True)\n",
    "# train_y.drop(columns=['symbol'], inplace=True)\n",
    "# test_X.drop(columns=['symbol'], inplace=True)\n",
    "# test_y.drop(columns=['symbol'], inplace=True)\n",
    "\n",
    "# train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "# test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "\n",
    "# Create and use the Cluster object\n",
    "cls = ch(train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'symbol']), train_y.drop(columns=['symbol']), \n",
    "                     test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'symbol']), test_y.drop(columns=['symbol']), \n",
    "                     eps=0.00001, mergeCluster=True, splitLargest=True)\n",
    "clustered_train_X, clustered_train_y, clustered_test_X, clustered_test_y = cls.cluster()\n",
    "\n",
    "# Access the clustering results\n",
    "feature_cluster_stats = cls.feature_cluster_stats\n",
    "y_cluster_stats = cls.y_cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t_X = clustered_train_X.to_numpy(), clustered_test_X.to_numpy()\n",
    "pred_col = test_y.columns[0]\n",
    "y = clustered_train_y[[c for c in clustered_train_y.columns if pred_col in c]].to_numpy()\n",
    "t_y = clustered_test_y[[c for c in clustered_test_y.columns if pred_col in c]].to_numpy()\n",
    "crisp_t_y = test_y[pred_col].to_numpy()\n",
    "\n",
    "final_model, final_r2, pred = train_model(\n",
    "    X, y, t_X, t_y, crisp_t_y,\n",
    "    **best_chromosome\n",
    ")\n",
    "print(\"Final model R2 score:\", final_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_log_returns = cls.deFuzzify(pred, pred_col)\n",
    "pred_log_returns = np.nan_to_num(pred_log_returns, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "pred_close = Utils.add_log_return_to_close(test_X['Close'].to_numpy(), pred_log_returns)\n",
    "target_close = Utils.add_log_return_to_close(test_X['Close'].to_numpy(), test_y[pred_col].to_numpy())\n",
    "close_r2_score = r2_score(target_close, pred_close)\n",
    "win_rate = Utils.calculate_win_rate(crisp_t_y, pred_log_returns)\n",
    "\n",
    "print(f\"Stock Price R2: {close_r2_score:.4f}\")\n",
    "print(f\"Win Rate: {100 * win_rate:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = DataHandler.getTickers(top100, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "# train_X, train_y, test_X, test_y = DataHandler.getTickers(top10, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "# train_X, train_y, test_X, test_y = DataHandler.getData(\"MS\", \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2023-01-01\", y_horizon)\n",
    "\n",
    "# train_X.drop(columns=['symbol'], inplace=True)\n",
    "# train_y.drop(columns=['symbol'], inplace=True)\n",
    "# test_X.drop(columns=['symbol'], inplace=True)\n",
    "# test_y.drop(columns=['symbol'], inplace=True)\n",
    "\n",
    "# train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "# test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "\n",
    "# Create and use the Cluster object\n",
    "cls = ch(train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume']), train_y, \n",
    "                     test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume']), test_y, \n",
    "                     eps=0.00001, mergeCluster=True, splitLargest=True)\n",
    "clustered_train_X, clustered_train_y, clustered_test_X, clustered_test_y = cls.cluster()\n",
    "\n",
    "# Access the clustering results\n",
    "feature_cluster_stats = cls.feature_cluster_stats\n",
    "y_cluster_stats = cls.y_cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t_X = clustered_train_X.to_numpy(), clustered_test_X.to_numpy()\n",
    "pred_col = test_y.columns[0]\n",
    "y = clustered_train_y[[c for c in clustered_train_y.columns if pred_col in c]].to_numpy()\n",
    "t_y = clustered_test_y[[c for c in clustered_test_y.columns if pred_col in c]].to_numpy()\n",
    "crisp_t_y = test_y[pred_col].to_numpy()\n",
    "\n",
    "model, r2, pred = train_model(\n",
    "    X, y, t_X, t_y,\n",
    "    crisp_t_y,\n",
    "    num_heads=1, \n",
    "    feed_forward_dim=1, \n",
    "    num_transformer_blocks=3, \n",
    "    mlp_units=64, \n",
    "    dropout_rate=0.1, \n",
    "    learning_rate=5e-06, \n",
    "    num_mlp_layers=3, \n",
    "    num_epochs=100, \n",
    "    activation_function=0,  # 0 for sigmoid, 1 for softmax\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_log_returns = cls.deFuzzify(pred, pred_col)\n",
    "pred_log_returns = np.nan_to_num(pred_log_returns, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "pred_close = Utils.add_log_return_to_close(test_X['Close'].to_numpy(), pred_log_returns)\n",
    "target_close = Utils.add_log_return_to_close(test_X['Close'].to_numpy(), test_y[pred_col].to_numpy())\n",
    "close_r2_score = r2_score(target_close, pred_close)\n",
    "win_rate = Utils.calculate_win_rate(crisp_t_y, pred_log_returns)\n",
    "\n",
    "print(f\"Stock Price R2: {close_r2_score:.4f}\")\n",
    "print(f\"Win Rate: {100 * win_rate:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.visualize_predictions_vs_target(pred_log_returns, crisp_t_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp1)",
   "language": "python",
   "name": "fyp1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
