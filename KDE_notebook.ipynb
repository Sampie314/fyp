{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from numpy import array, linspace\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from matplotlib.pyplot import plot\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams[\"figure.figsize\"] = (60, 40)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from handlers.DataHandler import DataHandler\n",
    "# from handlers.ClusterHandler import ClusterHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL VARIABLES ###\n",
    "y_horizon = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.signal import argrelextrema\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ClusterHandler:\n",
    "    def __init__(self, train_X, train_y, test_X, test_y, eps=0.0005, mergeCluster=True, pltKDE=False, pltNo=0, splitLargest=True):\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y\n",
    "        self.eps = eps\n",
    "        self.mergeCluster = mergeCluster\n",
    "        self.pltKDE = pltKDE\n",
    "        self.pltNo = pltNo\n",
    "        self.splitLargest = splitLargest\n",
    "\n",
    "        # Initialize dictionaries to store cluster statistics\n",
    "        self.feature_cluster_stats = {}\n",
    "        self.y_cluster_stats = {}\n",
    "\n",
    "    def cluster(self):\n",
    "        \"\"\"Main clustering method that processes all columns.\"\"\"\n",
    "        train_X_res = []\n",
    "        train_y_res = []\n",
    "        test_X_res = []\n",
    "        test_y_res = []\n",
    "\n",
    "        assert self.train_X.shape[1] == self.test_X.shape[1], \"Train and test data must have the same number of columns\"\n",
    "        assert self.train_y.shape[1] == self.test_y.shape[1], \"Train_y and test_y data must have the same number of columns\"\n",
    "        assert self.train_X.columns.tolist() == self.test_X.columns.tolist(), \"Train and test data must have the same columns\"\n",
    "        assert self.train_y.columns.tolist() == self.test_y.columns.tolist(), \"Train_y and test_y data must have the same columns\"\n",
    "\n",
    "        # Cluster each feature column\n",
    "        for column in self.train_X.columns:\n",
    "            temp, test_temp = self._process_column(column, is_target=False)\n",
    "            train_X_res.append(temp)\n",
    "            test_X_res.append(test_temp)\n",
    "\n",
    "        # Combine clustered results for features\n",
    "        train_X_res = pd.concat(train_X_res, axis=1)\n",
    "        test_X_res = pd.concat(test_X_res, axis=1)\n",
    "\n",
    "        # Cluster each target variable\n",
    "        for column in self.train_y.columns:\n",
    "            temp, test_temp = self._process_column(column, is_target=True)\n",
    "            train_y_res.append(temp)\n",
    "            test_y_res.append(test_temp)\n",
    "\n",
    "        # Combine clustered results for target variables\n",
    "        train_y_res = pd.concat(train_y_res, axis=1)\n",
    "        test_y_res = pd.concat(test_y_res, axis=1)\n",
    "        \n",
    "        return train_X_res, train_y_res, test_X_res, test_y_res\n",
    "\n",
    "    def _process_column(self, column_name, is_target=False):\n",
    "        \"\"\"Process a single column for clustering.\"\"\"\n",
    "        temp, testTemp, clsDic = self._cluster_column(column_name, is_target)\n",
    "\n",
    "        # Remove temporary columns\n",
    "        temp.drop([f\"Cluster_{column_name}\", \"Data\", \"Cluster_Mean\", \"Cluster_Std\"], axis=1, inplace=True)\n",
    "        testTemp.drop([\"Data\"], axis=1, inplace=True)\n",
    "\n",
    "        if is_target:\n",
    "            # temp.reset_index(drop=True, inplace=True)\n",
    "            temp.index = self.train_y.index\n",
    "            testTemp.index = self.test_y.index\n",
    "        else:\n",
    "            # testTemp.reset_index(drop=True, inplace=True)\n",
    "            testTemp.index = self.test_X.index\n",
    "            temp.index = self.train_X.index\n",
    "\n",
    "        # Append results\n",
    "        # toTrain.append(temp)\n",
    "        # toTest.append(testTemp)\n",
    "\n",
    "        # Store cluster statistics\n",
    "        if is_target:\n",
    "            self.y_cluster_stats[column_name] = clsDic\n",
    "        else:\n",
    "            self.feature_cluster_stats[column_name] = clsDic\n",
    "        return temp, testTemp\n",
    "\n",
    "    def _cluster_column(self, column_name, is_target=False):\n",
    "        \"\"\"Cluster a single column of data.\"\"\"\n",
    "        # Extract data\n",
    "        spl = (self.train_y if is_target else self.train_X)[column_name].to_numpy().reshape(-1, 1)\n",
    "        test_spl = (self.test_y if is_target else self.test_X)[column_name].to_numpy()\n",
    "\n",
    "        # Perform KDE and find splits\n",
    "        splits = self._kde_splits(spl)\n",
    "\n",
    "        # Assign initial clusters\n",
    "        cls1 = np.sum(spl.reshape(-1)[:, np.newaxis] >= splits, axis=1)\n",
    "\n",
    "        # Create temporary DataFrames\n",
    "        temp = pd.DataFrame({'Data': spl.reshape(-1), f'Cluster_{column_name}': cls1})\n",
    "        testTemp = pd.DataFrame({'Data': test_spl})\n",
    "\n",
    "        # Calculate initial cluster statistics\n",
    "        self._calculate_cluster_stats(temp, f'Cluster_{column_name}')\n",
    "\n",
    "        # Merge small clusters if enabled\n",
    "        if self.mergeCluster:\n",
    "            self._merge_clusters(temp, f'Cluster_{column_name}')\n",
    "\n",
    "        # Calculate membership functions\n",
    "        clsDic = self._calculate_memberships(temp, testTemp, f'Cluster_{column_name}', column_name, is_target)\n",
    "\n",
    "        print(f\"{column_name} had {len(clsDic)} clusters\")\n",
    "        return temp, testTemp, clsDic\n",
    "\n",
    "    def _kde_splits(self, spl):\n",
    "        \"\"\"Perform KDE and find splits in the data.\"\"\"\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=self.eps).fit(spl)\n",
    "        s = np.linspace(min(spl), max(spl))\n",
    "        e = kde.score_samples(s.reshape(-1, 1))\n",
    "        mi = argrelextrema(e, np.less)[0]\n",
    "        splits = s[mi].reshape(-1)\n",
    "\n",
    "        if self.splitLargest:\n",
    "            splits = self._split_largest_cluster(splits)\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def _split_largest_cluster(self, splits):\n",
    "        \"\"\"Split the largest cluster.\"\"\"\n",
    "        differences = np.abs(np.diff(splits))\n",
    "        max_diff_index = np.argmax(differences)\n",
    "        middle_element = (splits[max_diff_index] + splits[max_diff_index + 1]) / 2\n",
    "        return np.insert(splits, max_diff_index + 1, middle_element)\n",
    "\n",
    "    def _calculate_cluster_stats(self, df, cluster_col):\n",
    "        \"\"\"Calculate mean and std for each cluster.\"\"\"\n",
    "        for cluster_id, cluster_data in df.groupby(cluster_col):\n",
    "            df.loc[df[cluster_col] == cluster_id, 'Cluster_Mean'] = cluster_data['Data'].mean()\n",
    "            df.loc[df[cluster_col] == cluster_id, 'Cluster_Std'] = cluster_data['Data'].std(ddof=1)\n",
    "\n",
    "    def _merge_clusters(self, df, cluster_col):\n",
    "        \"\"\"Merge small clusters.\"\"\"\n",
    "        clusterNo = sorted(df[cluster_col].unique())\n",
    "        i, j = 0, 1\n",
    "        while i < len(clusterNo) and j < len(clusterNo):\n",
    "            lCls, rCls = clusterNo[i], clusterNo[j]\n",
    "            if len(df[df[cluster_col] == lCls]) < 5 or len(df[df[cluster_col] == rCls]) < 5:\n",
    "                df.loc[df[cluster_col] == rCls, cluster_col] = lCls\n",
    "                newMean = df[df[cluster_col] == lCls]['Data'].mean()\n",
    "                newStd = df[df[cluster_col] == lCls]['Data'].std()\n",
    "                df.loc[df[cluster_col] == lCls, 'Cluster_Mean'] = newMean\n",
    "                df.loc[df[cluster_col] == lCls, 'Cluster_Std'] = newStd\n",
    "                clusterNo.pop(j)\n",
    "            else:\n",
    "                i += 1\n",
    "                j += 1\n",
    "\n",
    "    def _calculate_memberships(self, temp, testTemp, cluster_col, column_name, is_target):\n",
    "        \"\"\"Calculate membership functions for each cluster.\"\"\"\n",
    "        clsDic = {}\n",
    "        for clsNo, cluster_data in enumerate(temp.groupby(cluster_col)):\n",
    "            _, data = cluster_data\n",
    "            sampleMean = data['Data'].mean()\n",
    "            sampleDevi = data['Data'].std(ddof=1)\n",
    "\n",
    "            # Handle prefix assignment\n",
    "            prefix = 'y' if is_target else column_name\n",
    "            pdf_col = f'PDF_{prefix}_{clsNo}'\n",
    "\n",
    "            temp[pdf_col] = temp[\"Data\"].apply(lambda x: self._membership(x, sampleMean, sampleDevi))\n",
    "            testTemp[pdf_col] = testTemp[\"Data\"].apply(lambda x: self._membership(x, sampleMean, sampleDevi))\n",
    "\n",
    "            clsDic[clsNo] = {'mean': sampleMean, 'std': sampleDevi}\n",
    "\n",
    "        return clsDic\n",
    "\n",
    "    def _membership(self, x, sampleMean, sampleDevi):\n",
    "        \"\"\"Calculate Gaussian membership.\"\"\"\n",
    "        return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
    "\n",
    "    def clusterGraph(self):\n",
    "        \"\"\"Generate cluster distribution graphs.\"\"\"\n",
    "        fig, axs = plt.subplots(12, 3, figsize=(45,55))  # 1 row, 2 columns\n",
    "        for z in range(0, 17): # 16 time steps, ignore t\n",
    "            a = np.where(self.means[z] == 0)[0]\n",
    "            if len(a)==0:\n",
    "                a = len(self.means[z])\n",
    "            else:\n",
    "                a = a[0]\n",
    "            for i in range(a):\n",
    "                axs[z//3][z%3].scatter(self.train[\"Close_t-{}\".format(z)], self.train[\"PDF_{}_{}\".format(z, i)])\n",
    "            axs[z//3][z%3].set_xlabel('Closing Value at t-{}'.format(z), fontsize = 20)\n",
    "            axs[z//3][z%3].set_ylabel('Membership Value', fontsize = 20)\n",
    "            axs[z//3][z%3].set_title('Cluster Distribution for t-{}'.format(z), fontsize = 30)\n",
    "            \n",
    "        for z in range(0, 16): # 15 time steps, ignore t\n",
    "            a = np.where(self.meansMomentum[z] == 0)[0]\n",
    "            if len(a)==0:\n",
    "                a = len(self.meansMomentum[z])\n",
    "            else:\n",
    "                a = a[0]\n",
    "            for i in range(a):\n",
    "                axs[(z+16)//3][(z+16)%3].scatter(self.train[\"Close_t-{}.2\".format(z)], self.train[\"PDF_{}.2_{}\".format(z, i)])\n",
    "            axs[(z+16)//3][(z+16)%3].set_xlabel('Closing Value at t-{}.2'.format(z), fontsize = 20)\n",
    "            axs[(z+16)//3][(z+16)%3].set_ylabel('Membership Value', fontsize = 20)\n",
    "            axs[(z+16)//3][(z+16)%3].set_title('Cluster Distribution for t-{}.2'.format(z), fontsize = 30)\n",
    "\n",
    "        for i in range(len(self.y_cluster_stats)):\n",
    "                axs[-1][-1].scatter(self.train[\"Close_t+{}\".format(self.yTarget)], self.train[\"PDF_y_{}\".format(i)])\n",
    "        axs[-1][-1].set_xlabel('Closing Value at t+{}'.format(self.yTarget), fontsize = 20)\n",
    "        axs[-1][-1].set_ylabel('Membership Value', fontsize = 20)\n",
    "        axs[-1][-1].set_title('Cluster Distribution for t-{}'.format(self.yTarget), fontsize = 30)\n",
    "        # plt.tight_layout()    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open had 16 clusters\n",
      "High had 15 clusters\n",
      "Low had 14 clusters\n",
      "Close had 19 clusters\n",
      "Volume had 4 clusters\n",
      "daily_log_return_0 had 8 clusters\n",
      "daily_log_return_1 had 8 clusters\n",
      "daily_log_return_2 had 8 clusters\n",
      "daily_log_return_3 had 8 clusters\n",
      "daily_log_return_4 had 8 clusters\n",
      "daily_log_return_5 had 8 clusters\n",
      "daily_log_return_6 had 8 clusters\n",
      "daily_log_return_7 had 8 clusters\n",
      "daily_log_return_8 had 8 clusters\n",
      "daily_log_return_9 had 8 clusters\n",
      "daily_log_return_10 had 8 clusters\n",
      "daily_log_return_11 had 8 clusters\n",
      "daily_log_return_12 had 8 clusters\n",
      "intraday_log_return_0 had 9 clusters\n",
      "intraday_log_return_1 had 9 clusters\n",
      "intraday_log_return_2 had 9 clusters\n",
      "intraday_log_return_3 had 9 clusters\n",
      "intraday_log_return_4 had 9 clusters\n",
      "intraday_log_return_5 had 9 clusters\n",
      "intraday_log_return_6 had 9 clusters\n",
      "intraday_log_return_7 had 9 clusters\n",
      "intraday_log_return_8 had 9 clusters\n",
      "intraday_log_return_9 had 9 clusters\n",
      "intraday_log_return_10 had 9 clusters\n",
      "intraday_log_return_11 had 9 clusters\n",
      "intraday_log_return_12 had 9 clusters\n",
      "overnight_log_return_0 had 4 clusters\n",
      "overnight_log_return_1 had 4 clusters\n",
      "overnight_log_return_2 had 4 clusters\n",
      "overnight_log_return_3 had 4 clusters\n",
      "overnight_log_return_4 had 4 clusters\n",
      "overnight_log_return_5 had 4 clusters\n",
      "overnight_log_return_6 had 4 clusters\n",
      "overnight_log_return_7 had 4 clusters\n",
      "overnight_log_return_8 had 4 clusters\n",
      "overnight_log_return_9 had 4 clusters\n",
      "overnight_log_return_10 had 4 clusters\n",
      "overnight_log_return_11 had 4 clusters\n",
      "overnight_log_return_12 had 4 clusters\n",
      "day_range_0 had 7 clusters\n",
      "day_range_1 had 7 clusters\n",
      "day_range_2 had 7 clusters\n",
      "day_range_3 had 7 clusters\n",
      "day_range_4 had 7 clusters\n",
      "day_range_5 had 7 clusters\n",
      "day_range_6 had 7 clusters\n",
      "day_range_7 had 7 clusters\n",
      "day_range_8 had 7 clusters\n",
      "day_range_9 had 7 clusters\n",
      "day_range_10 had 7 clusters\n",
      "day_range_11 had 7 clusters\n",
      "day_range_12 had 7 clusters\n",
      "notional_traded had 6 clusters\n",
      "notional_traded_change_0 had 2 clusters\n",
      "notional_traded_change_1 had 2 clusters\n",
      "notional_traded_change_2 had 2 clusters\n",
      "notional_traded_change_3 had 2 clusters\n",
      "notional_traded_change_4 had 2 clusters\n",
      "notional_traded_change_5 had 2 clusters\n",
      "notional_traded_change_6 had 2 clusters\n",
      "notional_traded_change_7 had 2 clusters\n",
      "notional_traded_change_8 had 2 clusters\n",
      "notional_traded_change_9 had 2 clusters\n",
      "notional_traded_change_10 had 2 clusters\n",
      "notional_traded_change_11 had 2 clusters\n",
      "notional_traded_change_12 had 2 clusters\n",
      "num_up_days_1m had 13 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n",
      "/var/folders/ng/6yknzn1x345dnygqqxr3qjw00000gn/T/ipykernel_69952/2222618881.py:180: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return math.exp(-((x - sampleMean)**2 / (2 * (sampleDevi**2))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m_mom had 9 clusters\n",
      "3m_mom had 15 clusters\n",
      "6m_mom had 15 clusters\n",
      "12m_mom had 16 clusters\n",
      "18m_mom had 16 clusters\n",
      "mom_change_1m_3m had 11 clusters\n",
      "mom_change_3m_6m had 16 clusters\n",
      "mom_change_6m_12m had 15 clusters\n",
      "y_log_return_0 had 8 clusters\n",
      "y_log_return_1 had 4 clusters\n",
      "y_log_return_2 had 10 clusters\n",
      "y_log_return_3 had 8 clusters\n",
      "y_log_return_4 had 5 clusters\n",
      "y_log_return_5 had 9 clusters\n",
      "y_log_return_6 had 8 clusters\n",
      "y_log_return_7 had 8 clusters\n",
      "y_log_return_8 had 11 clusters\n",
      "y_log_return_9 had 9 clusters\n",
      "y_log_return_10 had 10 clusters\n",
      "y_log_return_11 had 13 clusters\n",
      "y_log_return_12 had 10 clusters\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "train_X, train_y, test_X, test_y = DataHandler.getData(\"MS\", \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2023-01-01\", y_horizon)\n",
    "\n",
    "# Create and use the Cluster object\n",
    "cls = ClusterHandler(train_X, train_y, test_X, test_y, eps=0.00001, mergeCluster=True, splitLargest=True)\n",
    "clustered_train_X, clustered_train_y, clustered_test_X, clustered_test_y = cls.cluster()\n",
    "\n",
    "# Access the clustering results\n",
    "feature_cluster_stats = cls.feature_cluster_stats\n",
    "y_cluster_stats = cls.y_cluster_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads=10, ff_dim=32, num_transformer_blocks=4, mlp_units=256, dropout=0.25, noHiddenLayers = 1, sigmoidOrSoftmax = 0):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # Encoder layer\n",
    "        encoder_layers = TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=mlp_units)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=num_transformer_blocks)\n",
    "        \n",
    "        fcList = [torch.nn.Linear(input_dim, mlp_units), torch.nn.ReLU()]\n",
    "        for i in range(noHiddenLayers):\n",
    "            fcList.append(torch.nn.Linear(mlp_units, mlp_units//2))\n",
    "            fcList.append(torch.nn.ReLU())\n",
    "            mlp_units = mlp_units//2\n",
    "        fcList.append(torch.nn.Linear(mlp_units, output_dim))\n",
    "        if sigmoidOrSoftmax == 0:\n",
    "            fcList.append(torch.nn.Sigmoid())\n",
    "        else:\n",
    "            fcList.append(torch.nn.Softmax())\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(*fcList)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (seq_length, batch_size, input_dim)\n",
    "        \n",
    "        # Pass input through the transformer encoder\n",
    "        encoder_output = self.transformer_encoder(src)\n",
    "        encoder_output = encoder_output[:,:]\n",
    "        output = self.fc(encoder_output)\n",
    "        return output    \n",
    "    \n",
    "def increaseInstancesExtreme(train, thresholdToIncrease=0.03):\n",
    "    extraData = train[(train[f\"Close_t+{yTarget}\"]>thresholdToIncrease) | (train[f\"Close_t+{yTarget}\"]<-thresholdToIncrease)]\n",
    "    return pd.concat([train, extraData] ,axis = 0)\n",
    "\n",
    "def weightAvg(pred, noClusters, deltaOnly = True):\n",
    "    center = [i[0] for i in cls.yClsDic.values()]\n",
    "    std = [i[1] for i in cls.yClsDic.values()]\n",
    "    std = np.tile(center,(pred.shape[0],1))\n",
    "    centers = np.tile(center, (pred.shape[0], 1))\n",
    "    pred = np.nan_to_num(pred, nan=0, posinf=0, neginf=0)\n",
    "    denominator = pred\n",
    "    denominator = denominator.sum(axis=1, keepdims=True)\n",
    "    numerator = (pred * centers)\n",
    "    numerator = numerator.sum(axis = 1, keepdims = True)\n",
    "    result = numerator / denominator\n",
    "    result = np.squeeze(result)\n",
    "    if not deltaOnly:\n",
    "        return (result * cls.test.Close.to_numpy())+cls.test.Close.to_numpy()\n",
    "    else:\n",
    "        return (result * cls.test.Close.to_numpy())\n",
    "    \n",
    "def padData(X, X_test, padDim):\n",
    "    zeros_array = np.zeros((X.shape[0], padDim))\n",
    "    zeros_array_test = np.zeros((X_test.shape[0], padDim))\n",
    "\n",
    "    X = np.concatenate((X, zeros_array), axis=1)\n",
    "    X_test = np.concatenate((X_test, zeros_array_test), axis=1)\n",
    "    return X, X_test\n",
    "\n",
    "def testData(pred, Y_test):\n",
    "    res = (pred-Y_test)**2\n",
    "    res[np.isnan(res)]=0\n",
    "    mse = np.mean(res)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def train_ga_model(num_heads, feed_forward_dim, num_transformer_blocks, mlp_units, dropout_rate, learning_rate, num_mlp_layers, num_epochs, activation_function):\n",
    "    # Detect GPU availability\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    # Load and pad data\n",
    "    X, Y, X_test, Y_test = clustered_train_X.to_numpy(), clustered_train_y.to_numpy(), clustered_test_X.to_numpy(), clustered_test_y.to_numpy()\n",
    "    x_padded, x_test_padded = padData(X, X_test, math.ceil(X.shape[1] / num_heads) * num_heads - X.shape[1])\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TransformerModel(\n",
    "        input_dim=x_padded.shape[1], \n",
    "        output_dim=Y.shape[1],\n",
    "        num_heads=num_heads, \n",
    "        ff_dim=feed_forward_dim, \n",
    "        num_transformer_blocks=num_transformer_blocks, \n",
    "        mlp_units=mlp_units, \n",
    "        dropout=dropout_rate, \n",
    "        sigmoidOrSoftmax=activation_function\n",
    "    ).double()\n",
    "\n",
    "    # Send model to the detected device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set loss function and optimizer\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader for training\n",
    "    assert x_padded.shape[0] == Y.shape[0]\n",
    "    train_dataset = TensorDataset(torch.from_numpy(x_padded), torch.from_numpy(Y))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            # Move data to the correct device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_dataloader):.4f}', end=\" \")\n",
    "\n",
    "        # Evaluate on test data\n",
    "        test_dataset = TensorDataset(torch.from_numpy(x_test_padded), torch.from_numpy(Y_test))\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "        model.eval()\n",
    "        pred_list = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in test_dataloader:\n",
    "                pred = model(inputs.to(device)).cpu()\n",
    "                pred_list.append(pred)\n",
    "\n",
    "        mse = testData(np.concatenate(pred_list), Y_test)\n",
    "        # print(f'Test Data MSE: {mse:.7f}')\n",
    "\n",
    "        # Calculate R2 score\n",
    "        pred = np.concatenate(pred_list)\n",
    "        pred[np.isnan(pred)] = 0\n",
    "        res = weightAvg(pred, len(cls.yClsDic), isTestY=True)\n",
    "        res = np.nan_to_num(res, nan=0, posinf=0, neginf=0)\n",
    "        pred_closing = addResToClosing(cls, res)[:-yTarget]\n",
    "        actual_closing = cls.test.Close[yTarget:].to_numpy()\n",
    "        r2_score_value = r2_score(actual_closing, pred_closing)\n",
    "        print(\" R2 Score:\", r2_score_value)\n",
    "\n",
    "    return model, r2_score_value, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/dsenv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "x_padded, x_test_padded = train_ga_model(\n",
    "    num_heads=1, \n",
    "    feed_forward_dim=1, \n",
    "    num_transformer_blocks=3, \n",
    "    mlp_units=64, \n",
    "    dropout_rate=0.1, \n",
    "    learning_rate=5e-06, \n",
    "    num_mlp_layers=3, \n",
    "    num_epochs=50, \n",
    "    activation_function=0  # 0 for sigmoid, 1 for softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_padded == clustered_train_X.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 590)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4138, 590)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_train_X.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 590)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_test_X.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, r2, pred = train_ga_model(\n",
    "    num_heads=1, \n",
    "    feed_forward_dim=1, \n",
    "    num_transformer_blocks=3, \n",
    "    mlp_units=64, \n",
    "    dropout_rate=0.1, \n",
    "    learning_rate=5e-06, \n",
    "    num_mlp_layers=3, \n",
    "    num_epochs=50, \n",
    "    activation_function=0  # 0 for sigmoid, 1 for softmax\n",
    ")\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935833218015573"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(train_X['Close'].iloc[13:], train_X['Close'].shift(13).iloc[13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1999-07-06    25.664207\n",
       "1999-07-07    25.695587\n",
       "1999-07-08    25.774012\n",
       "1999-07-09    25.993652\n",
       "1999-07-12    25.601460\n",
       "                ...    \n",
       "2015-12-07    27.070307\n",
       "2015-12-08    26.586359\n",
       "2015-12-09    26.047758\n",
       "2015-12-10    26.086788\n",
       "2015-12-11    25.040825\n",
       "Name: Close, Length: 4138, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1999-07-06          NaN\n",
       "1999-07-07    25.664207\n",
       "1999-07-08    25.695587\n",
       "1999-07-09    25.774012\n",
       "1999-07-12    25.993652\n",
       "                ...    \n",
       "2015-12-07    27.569885\n",
       "2015-12-08    27.070307\n",
       "2015-12-09    26.586359\n",
       "2015-12-10    26.047758\n",
       "2015-12-11    26.086788\n",
       "Name: Close, Length: 4138, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X['Close'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
