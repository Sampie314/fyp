{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from numpy import array, linspace\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from matplotlib.pyplot import plot\n",
    "from scipy.signal import argrelextrema\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from handlers.DataHandler import DataHandler\n",
    "from handlers.GaussianMixtureHandler import GaussianMixtureHandler as ch \n",
    "from handlers import GAHandler\n",
    "from handlers import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL VARIABLES ###\n",
    "y_horizon = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'META', 'GOOGL', 'GOOG',\n",
    "       'LLY', 'JPM', 'AVGO', 'TSLA', 'UNH', 'XOM', 'V', 'PG', 'JNJ', 'MA',\n",
    "       'COST', 'HD', 'ABBV', 'WMT', 'MRK', 'NFLX', 'KO', 'BAC', 'ADBE',\n",
    "       'PEP', 'CVX', 'CRM', 'TMO', 'ORCL', 'LIN', 'AMD', 'ACN', 'MCD',\n",
    "       'ABT', 'CSCO', 'PM', 'WFC', 'IBM', 'TXN', 'QCOM', 'GE', 'DHR',\n",
    "       'VZ', 'INTU', 'AMGN', 'NOW', 'ISRG', 'NEE', 'SPGI', 'PFE', 'CAT',\n",
    "       'DIS', 'RTX', 'CMCSA', 'GS', 'UNP', 'T', 'AMAT', 'PGR',\n",
    "       'LOW', 'AXP', 'TJX', 'HON', 'BKNG', 'ELV', 'COP', 'SYK', 'MS',\n",
    "       'LMT', 'VRTX', 'BLK', 'REGN', 'MDT', 'BSX', 'PLD', 'CB', 'ETN',\n",
    "       'C', 'MMC', 'ADP', 'AMT', 'PANW', 'ADI', 'SBUX', 'MDLZ', 'CI',\n",
    "       'TMUS', 'FI', 'BMY', 'DE', 'GILD', 'BX', 'NKE', 'SO', 'LRCX', 'MU', 'KLAC', 'SCHW']\n",
    "\n",
    "top10 = top100[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-22 16:45:32,315 - handlers.DataHandler - INFO - Loading data for AAPL from cache\n",
      "2024-09-22 16:45:32,453 - handlers.DataHandler - INFO - Loading data for MSFT from cache\n",
      "2024-09-22 16:45:32,577 - handlers.DataHandler - INFO - Loading data for NVDA from cache\n",
      "2024-09-22 16:45:32,699 - handlers.DataHandler - INFO - Loading data for AMZN from cache\n",
      "2024-09-22 16:45:32,822 - handlers.DataHandler - INFO - Loading data for META from cache\n",
      "2024-09-22 16:45:32,940 - handlers.DataHandler - INFO - Loading data for GOOGL from cache\n",
      "2024-09-22 16:45:33,058 - handlers.DataHandler - INFO - Loading data for GOOG from cache\n",
      "2024-09-22 16:45:33,176 - handlers.DataHandler - INFO - Loading data for LLY from cache\n",
      "2024-09-22 16:45:33,299 - handlers.DataHandler - INFO - Loading data for JPM from cache\n",
      "2024-09-22 16:45:33,424 - handlers.DataHandler - INFO - Loading data for AVGO from cache\n",
      "2024-09-22 16:45:35,575 - handlers.GaussianMixtureHandler - INFO - daily_log_return_0 had 9 clusters\n",
      "2024-09-22 16:45:37,516 - handlers.GaussianMixtureHandler - INFO - daily_log_return_1 had 10 clusters\n",
      "2024-09-22 16:45:39,642 - handlers.GaussianMixtureHandler - INFO - daily_log_return_2 had 8 clusters\n",
      "2024-09-22 16:45:41,211 - handlers.GaussianMixtureHandler - INFO - daily_log_return_3 had 6 clusters\n",
      "2024-09-22 16:45:42,940 - handlers.GaussianMixtureHandler - INFO - daily_log_return_4 had 8 clusters\n",
      "2024-09-22 16:45:44,583 - handlers.GaussianMixtureHandler - INFO - daily_log_return_5 had 7 clusters\n",
      "2024-09-22 16:45:46,320 - handlers.GaussianMixtureHandler - INFO - daily_log_return_6 had 8 clusters\n",
      "2024-09-22 16:45:48,117 - handlers.GaussianMixtureHandler - INFO - daily_log_return_7 had 9 clusters\n",
      "2024-09-22 16:45:49,909 - handlers.GaussianMixtureHandler - INFO - daily_log_return_8 had 9 clusters\n",
      "2024-09-22 16:45:51,728 - handlers.GaussianMixtureHandler - INFO - daily_log_return_9 had 9 clusters\n",
      "2024-09-22 16:45:53,468 - handlers.GaussianMixtureHandler - INFO - daily_log_return_10 had 9 clusters\n",
      "2024-09-22 16:45:55,237 - handlers.GaussianMixtureHandler - INFO - daily_log_return_11 had 9 clusters\n",
      "2024-09-22 16:45:57,018 - handlers.GaussianMixtureHandler - INFO - daily_log_return_12 had 9 clusters\n",
      "2024-09-22 16:45:58,943 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_0 had 10 clusters\n",
      "2024-09-22 16:46:00,877 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_1 had 10 clusters\n",
      "2024-09-22 16:46:02,754 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_2 had 10 clusters\n",
      "2024-09-22 16:46:04,613 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_3 had 9 clusters\n",
      "2024-09-22 16:46:06,438 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_4 had 10 clusters\n",
      "2024-09-22 16:46:08,232 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_5 had 9 clusters\n",
      "2024-09-22 16:46:09,984 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_6 had 9 clusters\n",
      "2024-09-22 16:46:11,798 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_7 had 9 clusters\n",
      "2024-09-22 16:46:13,580 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_8 had 9 clusters\n",
      "2024-09-22 16:46:15,447 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_9 had 10 clusters\n",
      "2024-09-22 16:46:17,232 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_10 had 9 clusters\n",
      "2024-09-22 16:46:19,016 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_11 had 9 clusters\n",
      "2024-09-22 16:46:20,804 - handlers.GaussianMixtureHandler - INFO - intraday_log_return_12 had 9 clusters\n",
      "2024-09-22 16:46:22,625 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_0 had 9 clusters\n",
      "2024-09-22 16:46:24,429 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_1 had 9 clusters\n",
      "2024-09-22 16:46:26,140 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_2 had 7 clusters\n",
      "2024-09-22 16:46:27,728 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_3 had 6 clusters\n",
      "2024-09-22 16:46:29,597 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_4 had 9 clusters\n",
      "2024-09-22 16:46:31,408 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_5 had 9 clusters\n",
      "2024-09-22 16:46:33,200 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_6 had 8 clusters\n",
      "2024-09-22 16:46:35,112 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_7 had 9 clusters\n",
      "2024-09-22 16:46:36,867 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_8 had 8 clusters\n",
      "2024-09-22 16:46:38,684 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_9 had 9 clusters\n",
      "2024-09-22 16:46:40,233 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_10 had 6 clusters\n",
      "2024-09-22 16:46:41,967 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_11 had 8 clusters\n",
      "2024-09-22 16:46:43,813 - handlers.GaussianMixtureHandler - INFO - overnight_log_return_12 had 9 clusters\n",
      "2024-09-22 16:46:45,411 - handlers.GaussianMixtureHandler - INFO - day_range_0 had 10 clusters\n",
      "2024-09-22 16:46:46,922 - handlers.GaussianMixtureHandler - INFO - day_range_1 had 9 clusters\n",
      "2024-09-22 16:46:48,532 - handlers.GaussianMixtureHandler - INFO - day_range_2 had 10 clusters\n",
      "2024-09-22 16:46:50,161 - handlers.GaussianMixtureHandler - INFO - day_range_3 had 10 clusters\n",
      "2024-09-22 16:46:51,698 - handlers.GaussianMixtureHandler - INFO - day_range_4 had 9 clusters\n",
      "2024-09-22 16:46:53,297 - handlers.GaussianMixtureHandler - INFO - day_range_5 had 10 clusters\n",
      "2024-09-22 16:46:54,930 - handlers.GaussianMixtureHandler - INFO - day_range_6 had 10 clusters\n",
      "2024-09-22 16:46:56,432 - handlers.GaussianMixtureHandler - INFO - day_range_7 had 9 clusters\n",
      "2024-09-22 16:46:58,041 - handlers.GaussianMixtureHandler - INFO - day_range_8 had 10 clusters\n",
      "2024-09-22 16:46:59,660 - handlers.GaussianMixtureHandler - INFO - day_range_9 had 10 clusters\n",
      "2024-09-22 16:47:01,268 - handlers.GaussianMixtureHandler - INFO - day_range_10 had 10 clusters\n",
      "2024-09-22 16:47:02,857 - handlers.GaussianMixtureHandler - INFO - day_range_11 had 10 clusters\n",
      "2024-09-22 16:47:04,442 - handlers.GaussianMixtureHandler - INFO - day_range_12 had 10 clusters\n",
      "2024-09-22 16:47:05,864 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_0 had 5 clusters\n",
      "2024-09-22 16:47:07,484 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_1 had 7 clusters\n",
      "2024-09-22 16:47:09,119 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_2 had 7 clusters\n",
      "2024-09-22 16:47:10,442 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_3 had 4 clusters\n",
      "2024-09-22 16:47:11,888 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_4 had 5 clusters\n",
      "2024-09-22 16:47:13,493 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_5 had 7 clusters\n",
      "2024-09-22 16:47:14,899 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_6 had 5 clusters\n",
      "2024-09-22 16:47:16,328 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_7 had 5 clusters\n",
      "2024-09-22 16:47:17,741 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_8 had 5 clusters\n",
      "2024-09-22 16:47:19,358 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_9 had 7 clusters\n",
      "2024-09-22 16:47:20,814 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_10 had 5 clusters\n",
      "2024-09-22 16:47:22,228 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_11 had 5 clusters\n",
      "2024-09-22 16:47:23,668 - handlers.GaussianMixtureHandler - INFO - notional_traded_change_12 had 5 clusters\n",
      "2024-09-22 16:47:25,406 - handlers.GaussianMixtureHandler - INFO - num_up_days_1m had 10 clusters\n",
      "2024-09-22 16:47:26,845 - handlers.GaussianMixtureHandler - INFO - 1m_mom had 4 clusters\n",
      "2024-09-22 16:47:28,095 - handlers.GaussianMixtureHandler - INFO - 3m_mom had 3 clusters\n",
      "2024-09-22 16:47:29,667 - handlers.GaussianMixtureHandler - INFO - 6m_mom had 7 clusters\n",
      "2024-09-22 16:47:31,266 - handlers.GaussianMixtureHandler - INFO - 12m_mom had 7 clusters\n",
      "2024-09-22 16:47:32,993 - handlers.GaussianMixtureHandler - INFO - 18m_mom had 9 clusters\n",
      "2024-09-22 16:47:34,518 - handlers.GaussianMixtureHandler - INFO - mom_change_1m_3m had 6 clusters\n",
      "2024-09-22 16:47:36,110 - handlers.GaussianMixtureHandler - INFO - mom_change_3m_6m had 7 clusters\n",
      "2024-09-22 16:47:37,789 - handlers.GaussianMixtureHandler - INFO - mom_change_6m_12m had 8 clusters\n"
     ]
    }
   ],
   "source": [
    "# train_X, train_y, test_X, test_y = DataHandler.getTickers(top100, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "train_X, train_y, test_X, test_y = DataHandler.getTickers(top10, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "# train_X, train_y, test_X, test_y = DataHandler.getData(\"MS\", \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2023-01-01\", y_horizon)\n",
    "\n",
    "# train_X.drop(columns=['symbol'], inplace=True)\n",
    "# train_y.drop(columns=['symbol'], inplace=True)\n",
    "# test_X.drop(columns=['symbol'], inplace=True)\n",
    "# test_y.drop(columns=['symbol'], inplace=True)\n",
    "\n",
    "# train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "# test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "\n",
    "stationary_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'notional_traded']\n",
    "\n",
    "# Create and use the Cluster object\n",
    "cls = ch(train_X.drop(columns=stationary_cols + ['symbol']), train_y.drop(columns=['symbol']), \n",
    "                     test_X.drop(columns=stationary_cols + ['symbol']), test_y.drop(columns=['symbol']), \n",
    "                     )\n",
    "\n",
    "clustered_train_X, clustered_train_y, clustered_test_X, clustered_test_y = cls.cluster()\n",
    "\n",
    "# Access the clustering results\n",
    "feature_cluster_stats = cls.feature_cluster_stats\n",
    "y_cluster_stats = cls.y_cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_train_X.drop(columns=clustered_train_X.columns[clustered_train_X.isna().any(axis=0)], inplace=True)\n",
    "clustered_test_X.drop(columns=clustered_test_X.columns[clustered_test_X.isna().any(axis=0)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_train_X.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_test_X.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clustered_test_X.columns == clustered_train_X.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads=10, ff_dim=32, num_transformer_blocks=4, mlp_units=256, dropout=0.25, noHiddenLayers=1, sigmoidOrSoftmax=0):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # print(f\"Initializing TransformerModel with input_dim: {input_dim}, output_dim: {output_dim}\")\n",
    "\n",
    "        # # Ensure input_dim is divisible by num_heads\n",
    "        # if input_dim % num_heads != 0:\n",
    "        #     new_input_dim = (input_dim // num_heads + 1) * num_heads\n",
    "        #     print(f\"Adjusting input_dim from {input_dim} to {new_input_dim} to be divisible by num_heads ({num_heads})\")\n",
    "        #     input_dim = new_input_dim\n",
    "\n",
    "        # Encoder layer with ff_dim and dropout\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=num_transformer_blocks)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # MLP layers\n",
    "        fcList = [torch.nn.Linear(input_dim, mlp_units), torch.nn.ReLU(), self.dropout]\n",
    "        for i in range(noHiddenLayers):\n",
    "            fcList.extend([\n",
    "                torch.nn.Linear(mlp_units, mlp_units//2),\n",
    "                torch.nn.ReLU(),\n",
    "                self.dropout\n",
    "            ])\n",
    "            mlp_units = mlp_units//2\n",
    "        fcList.append(torch.nn.Linear(mlp_units, output_dim))\n",
    "        \n",
    "        # Output activation\n",
    "        if sigmoidOrSoftmax == 0:\n",
    "            fcList.append(torch.nn.Sigmoid())\n",
    "        else:\n",
    "            fcList.append(torch.nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(*fcList)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_length, input_dim)\n",
    "        \n",
    "        # Pass input through the transformer encoder\n",
    "        encoder_output = self.transformer_encoder(src)\n",
    "        \n",
    "        # Apply dropout after the transformer encoder\n",
    "        encoder_output = self.dropout(encoder_output)\n",
    "        \n",
    "        # Pass through the MLP layers\n",
    "        output = self.fc(encoder_output)\n",
    "        return output\n",
    "    \n",
    "def increaseInstancesExtreme(train, thresholdToIncrease=0.03):\n",
    "    extraData = train[(train[f\"Close_t+{yTarget}\"]>thresholdToIncrease) | (train[f\"Close_t+{yTarget}\"]<-thresholdToIncrease)]\n",
    "    return pd.concat([train, extraData] ,axis = 0)\n",
    "\n",
    "def train_model(X, Y, X_test, Y_test, # fuzzified inputs\n",
    "                   Y_test_raw, # crisp value\n",
    "                   num_heads, feed_forward_dim, num_transformer_blocks, mlp_units, dropout_rate, \n",
    "                   learning_rate, num_mlp_layers, num_epochs, activation_function, batch_size):\n",
    "\n",
    "    OUTPUT_FREQ = 50\n",
    "    # Detect GPU availability\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    # Load and pad data\n",
    "    x_padded, x_test_padded = Utils.padData(X, X_test, math.ceil(X.shape[1] / num_heads) * num_heads - X.shape[1])\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TransformerModel(\n",
    "        input_dim=x_padded.shape[1], \n",
    "        output_dim=Y.shape[1],\n",
    "        num_heads=num_heads, \n",
    "        ff_dim=feed_forward_dim, \n",
    "        num_transformer_blocks=num_transformer_blocks, \n",
    "        mlp_units=mlp_units, \n",
    "        dropout=dropout_rate, \n",
    "        sigmoidOrSoftmax=activation_function\n",
    "    ).double()\n",
    "\n",
    "    # Send model to the detected device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set loss function and optimizer\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader for training\n",
    "    assert x_padded.shape[0] == Y.shape[0]\n",
    "    train_dataset = TensorDataset(torch.from_numpy(x_padded), torch.from_numpy(Y))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            # Move data to the correct device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Collect predictions and targets for R² score computation\n",
    "            all_preds.append(outputs.detach().cpu().numpy())\n",
    "            all_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "        # Concatenate all batch predictions and targets\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "        if epoch % OUTPUT_FREQ == 0:\n",
    "            # Compute R² score\n",
    "            r2 = r2_score(all_targets, all_preds)\n",
    "            log_return_r2 = eval_model(model, x_test_padded, Y_test, Y_test_raw)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss / len(train_dataloader):.4f} | Train Cluster R² Score: {r2:.4f} | Test Log Return R² Score: {log_return_r2:.4f}')\n",
    "\n",
    "    # return all_preds, all_targets\n",
    "\n",
    "    # Evaluate on test data\n",
    "    test_dataset = TensorDataset(torch.from_numpy(x_test_padded), torch.from_numpy(Y_test))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=512*4, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_dataloader:\n",
    "            pred = model(inputs.to(device)).cpu()\n",
    "            pred_list.append(pred)\n",
    "\n",
    "    mse = Utils.testData(np.concatenate(pred_list), Y_test)\n",
    "    # print(f'Test Data MSE: {mse:.7f}')\n",
    "\n",
    "    # Calculate R2 score\n",
    "    pred = np.concatenate(pred_list)\n",
    "    pred[np.isnan(pred)] = 0\n",
    "\n",
    "    pred_log_returns = cls.deFuzzify(pred, pred_col)\n",
    "    pred_log_returns = np.nan_to_num(pred_log_returns, nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    # pred_closing = addResToClosing(cls, res)[:-yTarget]\n",
    "    # actual_closing = cls.test.Close[yTarget:].to_numpy()\n",
    "    # r2_score_value = r2_score(actual_closing, pred_closing)\n",
    "    \n",
    "    r2_score_value = r2_score(Y_test_raw, pred_log_returns)\n",
    "    print(\"Test R2 Score:\", r2_score_value)\n",
    "\n",
    "    return model, r2_score_value, pred\n",
    "\n",
    "def eval_model(model, X: np.array, Y: np.array, Y_crisp: np.array):\n",
    "    \"\"\"\n",
    "    X & Y are fuzzified inputs\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(f\"Using {device}\")    \n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X), torch.from_numpy(Y))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=512*4, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_dataloader:\n",
    "            pred = model(inputs.to(device)).cpu()\n",
    "            pred_list.append(pred)\n",
    "\n",
    "    # mse = Utils.testData(np.concatenate(pred_list), Y_test)\n",
    "    # print(f'Test Data MSE: {mse:.7f}')\n",
    "\n",
    "    # Calculate R2 score\n",
    "    pred = np.concatenate(pred_list)\n",
    "    pred[np.isnan(pred)] = 0\n",
    "\n",
    "    pred_log_returns = cls.deFuzzify(pred, pred_col)\n",
    "    pred_log_returns = np.nan_to_num(pred_log_returns, nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    r2_score_value = Utils.custom_r2_score(Y_crisp, pred_log_returns)\n",
    "    # print(\"Test R2 Score:\", r2_score_value)    \n",
    "    return r2_score_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_col = test_y.columns[0]\n",
    "\n",
    "train_val_split = 0.7\n",
    "unique_train_dates = clustered_train_X.index.unique()\n",
    "split_date = unique_train_dates[int(len(unique_train_dates) * train_val_split)]\n",
    "\n",
    "\n",
    "X = clustered_train_X.loc[clustered_train_X.index < split_date].to_numpy()\n",
    "val_X = clustered_train_X.loc[clustered_train_X.index >= split_date].to_numpy()\n",
    "\n",
    "y = clustered_train_y[[c for c in clustered_train_y.columns if pred_col in c]].loc[clustered_train_y.index < split_date].to_numpy()\n",
    "val_y = clustered_train_y[[c for c in clustered_train_y.columns if pred_col in c]].loc[clustered_train_y.index >= split_date].to_numpy()\n",
    "crisp_val_y = train_y[pred_col].loc[train_y.index >= split_date].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the genetic algorithm\n",
    "best_chromosome, best_fitness = GAHandler.genetic_algorithm(\n",
    "    train_model, X, y, val_X, val_y, crisp_val_y, \n",
    "    population_size=50, final_population_size=5, generations=10, elite_size=2\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the results\n",
    "print(\"Best hyperparameters:\", best_chromosome)\n",
    "print(\"Best R2 score:\", best_fitness)\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "print(f\"Execution time: {execution_time/60:.4f} minutes\")\n",
    "\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "# final_model, final_r2, final_pred = train_model(\n",
    "#     X, y, t_X, t_y, crisp_t_y,\n",
    "#     **best_chromosome\n",
    "# )\n",
    "# print(\"Final model R2 score:\", final_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", best_chromosome)\n",
    "print(\"Best R2 score:\", best_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_chromosome = {'num_heads': 4, 'feed_forward_dim': 128, 'num_transformer_blocks': 2, \n",
    "                   'mlp_units': 256, 'dropout_rate': 0.1, 'learning_rate': 5e-06, \n",
    "                   'num_mlp_layers': 5, 'num_epochs': 200, 'activation_function': 0, 'batch_size': 1024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_chromosome['batch_size'] = 256\n",
    "# best_chromosome['num_epochs'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_X, train_y, test_X, test_y = DataHandler.getTickers(top100, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "train_X, train_y, test_X, test_y = DataHandler.getTickers(top10, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "# train_X, train_y, test_X, test_y = DataHandler.getData(\"MS\", \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2023-01-01\", y_horizon)\n",
    "\n",
    "# train_X.drop(columns=['symbol'], inplace=True)\n",
    "# train_y.drop(columns=['symbol'], inplace=True)\n",
    "# test_X.drop(columns=['symbol'], inplace=True)\n",
    "# test_y.drop(columns=['symbol'], inplace=True)\n",
    "\n",
    "# train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "# test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "\n",
    "# Create and use the Cluster object\n",
    "cls = ch(train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'symbol']), train_y.drop(columns=['symbol']), \n",
    "                     test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'symbol']), test_y.drop(columns=['symbol']), \n",
    "                     eps=0.00001, mergeCluster=True, splitLargest=True)\n",
    "clustered_train_X, clustered_train_y, clustered_test_X, clustered_test_y = cls.cluster()\n",
    "\n",
    "# Access the clustering results\n",
    "feature_cluster_stats = cls.feature_cluster_stats\n",
    "y_cluster_stats = cls.y_cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t_X = clustered_train_X.to_numpy(), clustered_test_X.to_numpy()\n",
    "pred_col = test_y.columns[0]\n",
    "y = clustered_train_y[[c for c in clustered_train_y.columns if pred_col in c]].to_numpy()\n",
    "t_y = clustered_test_y[[c for c in clustered_test_y.columns if pred_col in c]].to_numpy()\n",
    "crisp_t_y = test_y[pred_col].to_numpy()\n",
    "\n",
    "final_model, final_r2, pred = train_model(\n",
    "    X, y, t_X, t_y, crisp_t_y,\n",
    "    **best_chromosome\n",
    ")\n",
    "print(\"Final model R2 score:\", final_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_log_returns = cls.deFuzzify(pred, pred_col)\n",
    "pred_log_returns = np.nan_to_num(pred_log_returns, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "pred_close = Utils.add_log_return_to_close(test_X['Close'].to_numpy(), pred_log_returns)\n",
    "target_close = Utils.add_log_return_to_close(test_X['Close'].to_numpy(), test_y[pred_col].to_numpy())\n",
    "close_r2_score = r2_score(target_close, pred_close)\n",
    "win_rate = Utils.calculate_win_rate(crisp_t_y, pred_log_returns)\n",
    "\n",
    "print(f\"Stock Price R2: {close_r2_score:.4f}\")\n",
    "print(f\"Win Rate: {100 * win_rate:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = DataHandler.getTickers(top100, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "# train_X, train_y, test_X, test_y = DataHandler.getTickers(top10, \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2024-01-01\", y_horizon)\n",
    "# train_X, train_y, test_X, test_y = DataHandler.getData(\"MS\", \"1998-01-01\", \"2015-12-31\", \"2016-01-01\", \"2023-01-01\", y_horizon)\n",
    "\n",
    "# train_X.drop(columns=['symbol'], inplace=True)\n",
    "# train_y.drop(columns=['symbol'], inplace=True)\n",
    "# test_X.drop(columns=['symbol'], inplace=True)\n",
    "# test_y.drop(columns=['symbol'], inplace=True)\n",
    "\n",
    "# train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "# test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "\n",
    "# Create and use the Cluster object\n",
    "cls = ch(train_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume']), train_y, \n",
    "                     test_X.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume']), test_y, \n",
    "                     eps=0.00001, mergeCluster=True, splitLargest=True)\n",
    "clustered_train_X, clustered_train_y, clustered_test_X, clustered_test_y = cls.cluster()\n",
    "\n",
    "# Access the clustering results\n",
    "feature_cluster_stats = cls.feature_cluster_stats\n",
    "y_cluster_stats = cls.y_cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t_X = clustered_train_X.to_numpy(), clustered_test_X.to_numpy()\n",
    "pred_col = test_y.columns[0]\n",
    "y = clustered_train_y[[c for c in clustered_train_y.columns if pred_col in c]].to_numpy()\n",
    "t_y = clustered_test_y[[c for c in clustered_test_y.columns if pred_col in c]].to_numpy()\n",
    "crisp_t_y = test_y[pred_col].to_numpy()\n",
    "\n",
    "model, r2, pred = train_model(\n",
    "    X, y, t_X, t_y,\n",
    "    crisp_t_y,\n",
    "    num_heads=1, \n",
    "    feed_forward_dim=1, \n",
    "    num_transformer_blocks=3, \n",
    "    mlp_units=64, \n",
    "    dropout_rate=0.1, \n",
    "    learning_rate=5e-06, \n",
    "    num_mlp_layers=3, \n",
    "    num_epochs=100, \n",
    "    activation_function=0,  # 0 for sigmoid, 1 for softmax\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_log_returns = cls.deFuzzify(pred, pred_col)\n",
    "pred_log_returns = np.nan_to_num(pred_log_returns, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "pred_close = Utils.add_log_return_to_close(test_X['Close'].to_numpy(), pred_log_returns)\n",
    "target_close = Utils.add_log_return_to_close(test_X['Close'].to_numpy(), test_y[pred_col].to_numpy())\n",
    "close_r2_score = r2_score(target_close, pred_close)\n",
    "win_rate = Utils.calculate_win_rate(crisp_t_y, pred_log_returns)\n",
    "\n",
    "print(f\"Stock Price R2: {close_r2_score:.4f}\")\n",
    "print(f\"Win Rate: {100 * win_rate:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.visualize_predictions_vs_target(pred_log_returns, crisp_t_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp1)",
   "language": "python",
   "name": "fyp1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
